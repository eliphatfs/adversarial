\documentclass{article}
\usepackage{homework}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\usepackage[ruled, vlined]{algorithm2e} % 算法与伪代码

\title{Baseline is All You Need: the Story of Us Failing to Outperform Project Baselines}
\author{时若曦-519030910373 \quad 杨博睿-519030910374}
\date{}

\begin{document}
\maketitle
\section{Introduction}\label{sec:Intro}


\section{Related Work}\label{sec:RelatedWork}
    To cope with security risks posed by adversarial examples, many novel defensive methods have been proposed, including adversarial training, hypersphere embedding and gradient obfuscation. (However, gradient obfuscation proves to be vulnerable under specifically-designed attacks.)

    We detail two new related techniques in adversarial defense.
    \paragraph{TRADES.} TRADES modifies the cross-entropy loss in adversarial training to achieve a trade-off between accuracy and model robustness.
    \begin{equation}\label{eq:trades}
        \min_{\mathbf{w}} \frac{1}{n}\sum_{i=1}^n \left( \mathcal{L}_{CE}\left( f_{\mathbf{w}}(\mathbf{x}_i), y_i \right) + \mathcal{L}_{KL}\left(f_{\mathbf{w}}(\mathbf{x}_i), f_{\mathbf{w}}(\mathbf{x}_i^{adv})\right) \right)
    \end{equation}
    where $\mathbf{x}^{adv}$ is the adversarial example, $\mathcal{L}_{CE}(\cdot)$ and $\mathcal{L}_{KL}(\cdot)$ are cross-entropy loss and Kullback-Liebler Divergence, repectively.

    \paragraph{Adversarial Weight Perturbation.} Adversarial Weight Perturbation (AWP) aims to narrow the robust generalization gap of AT models. It flattens the ``weight-loss landscape'' by adding extra perturbations to model parameters during adversarial training. This is based on an observation that models with good performance on unforeseen test data tend to have flatter weight-loss landscape. AWP solves the optimization problem in (\ref{eq:awp}).
    \begin{equation}\label{eq:awp}
        \min_{\mathbf{w}} \max_{\mathbf{v} \in \mathcal{V}} \frac{1}{n}\sum_{i=1}^n \max_{\|\mathbf{x}^{adv} - \mathbf{x}\|_p \le \epsilon} l(f_{\mathbf{f}+\mathbf{v}}(\mathbf{x}^{adv}_i), y_i)
    \end{equation}
    where $l(\cdot)$ is a loss function, $\mathbf{v}$ is the perturbation in model weight and $\mathcal{V}$ is the feasible region of weight perturbation.
 

\section{Frank-Wolfe Attack}\label{sec:AttackMethodology}


\section{FW-based Defense}\label{sec:DefenseMethodology}
    In this section, we detail our design of adversarial defense. Since we fail to come up with innovative fancy defense ideas, we fallback to traditional adversarial training approaches, and incorporated our proposed Frank-Wolfe methods into adversarial training.

    \subsection{Frank-Wolfe Defense}
        We utilize the proposed FW-AdAmp method and improved adversarial training by substituting FW attack for standard PGD attacks in the adversarial training process. Let $\mathbf{x}^{adv}$ denote the adversarial example generated by FW-AdAmp, then we train a robust model by solving the optimization in (\ref{eq:fwat}).
        \begin{equation}\label{eq:fwat}
            \min_{\mathbf{w}} \frac{1}{n}\sum_{i=1}^n \mathcal{L}_{CE}(f_{\mathbf{w}}(\mathbf{x}^{adv}_i), y_i))
        \end{equation}

    \subsection{Combination with Other AT Methods}
        FW attack can be flexibly integrated with other AT methods. In this section, we introduce the combination of our proposed FW method with AWP and TRADES.

        \paragraph{Combination with Adversarial Weight Perturbation.} The role of FW attack is similar to that of a PGD attack in the AT-AWP framework proposed by Wu et al. Therefore to combine FW attack with AWP, we simply change the attack method from PGD to FW in the AT-AWP.

        \paragraph{Combination with TRADES.} The combination with TRADES is also straightforward: by changing the loss from cross-entropy to TRADES loss given in (\ref{eq:trades}).

\section{Results}\label{sec:Results}
    \subsection{Attack}\label{subs:AttackExp}
    \begin{table}[h]
        \centering
        \begin{tabular}{cccccccc}
        \hline
            Method & M1 & M2 & M3 & M4 & M5 & M6 & BWD/FWD \\ \hline
            No attack & 94.29 & 83.02 & 80.33 & 84.92 & 81.43 & 88.25 & - \\
            PGD20 & 0.04 & 51.29 & 65.15 & 56.18 & 54.82 & 64.34 & 20/0 \\
            DeepFool & 0.02 & 48.05 & 31.60 & 53.81 & 52.55 & 60.99 & 200/20 \\
            Sobol Sampling & 75.30 & 79.96 & 60.52 & 82.33 & U & U & 0/200 \\
            ReLeak-PGD & 5.36 & U & 19.97 & 83.20 & U & 86.80 & 20/0 \\
            Krylov & U & U & U & 56.03 & U & 65.10 & 20/20 \\
            F-W & 0.00 & 50.25 & 63.12 & 55.30 & 53.46 & 63.79 & 20/0 \\
            F-W-Amp & 60.32 & 48.11 & 56.34 & 53.50 & 53.00 & 60.92 & 20/0 \\
            F-W-AdAmp & 0.00 & 47.79 & 45.00 & 53.27 & 52.35 & 60.54 & 20/0 \\ \hline
        \end{tabular}
        \caption{Results of 6 baseline models, under various attack attempts.}
    \end{table}
    \subsection{Defense}\label{subs:DefenseExp}
        \subsubsection{Experiment Setup}\label{subs:DefenseExpSetup}
            To evaluate our defense method, we train several ResNet (and its variations) models using our proposed FW defense with weight perturbation (referred to as FW-AWP in the following part). We also train ResNet-18 and Wide ResNet-34-10 using FW-AWP with TRADES loss for parallel comparision with model 6. All defense models are trained for 100 epochs, using SGD optimizer with momentum 0.9 and weight decay $5\times 10^{-4}$. We set the learning rate to 0.1, which is mutiplied by 0.1 at 75th and 90th epoch. For Adversarial Weight Perturbation, we set the parameter $\gamma$ to be $0.1$ and set a warm up of 10 epochs. For TRADES loss, we set the hyperparameter $\beta = 1/\lambda = 6.0$.

            We use a PGD-10 AT ResNet-18 model as our baseline. The training framework is provided by TA. The PGD used in AT has a step size of 0.007 and perturbation steps of 10. 

            All models are trained on 2 NVIDIA GTX 1080Ti GPUs.

        \subsubsection{Benchmarking on Different Models}\label{subs:DefenseModelBenchmarking}
            \begin{table}[h]
                \centering
                \begin{tabular}{cccccc}
                    \hline
                    Models                         & Loss   & Time      & Natural & PGD20 &  FW  \\ \hline
                    \multirow{2}{*}{ResNet18}      & AT     & 6.06 hrs  & 79.48 & 55.88 & 49.98 \\
                                                   & TRADES & 7.01 hrs  & 78.35 & 55.93 & 49.04 \\ \hline
                    \multirow{2}{*}{Wide ResNet28} & AT     & 25.61 hrs & \textbf{84.75} & 59.39 & \textbf{55.34} \\
                                                   & TRADES & 29.82 hrs & 82.17 & \textbf{60.02} & 53.65 \\ \hline
                    ResNet34                       & AT     & 10.38 hrs & 81.74 & 57.99 & 52.91 \\ \hline
                    PreAct ResNet18                & AT     & 5.92 hrs  & 80.41 & 55.76 & 50.14 \\ \hline
                    PreAct ResNet34                & AT     & 10.34 hrs & 81.86 & 57.90 & 53.80 \\ \hline
                \end{tabular}
                \caption{Benchmarking results of different models trained with Frank-Wolfe method with AWP, under PGD20 and Frank-Wolfe-20 attack. Evaluation is run on test set of CIFAR-10. Models use weights of their \emph{best} epoch.}
                \label{table:DefenseBenchmarkingResults}
            \end{table}
            Table \ref{table:DefenseBenchmarkingResults} reports the total training time and accuracy of different models, under both PGD20 attack and proposed Frank-Wolfe Attack with AWP.



\section{Conclusion}\label{sec:Conclusion}
We sugguest using model 6.

\end{document}